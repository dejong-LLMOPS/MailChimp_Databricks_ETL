{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49a6469c-0987-454f-be68-07cf336666d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# mailchimp_bronze_etl.ipynb\n",
    "\n",
    "# 1) Notebook Configuration & Setup\n",
    "\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import re\n",
    "import logging\n",
    "import sys\n",
    "import traceback\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List, Dict\n",
    "\n",
    "import mailchimp_marketing as MailchimpMarketing\n",
    "from mailchimp_marketing.api_client import ApiClientError\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "\n",
    "# 1.1) Logging Setup (Databricks-friendly)\n",
    "\n",
    "logger = logging.getLogger(\"MailchimpBronzeETL\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Prevent duplicate handlers if re-run\n",
    "if logger.hasHandlers():\n",
    "    logger.handlers.clear()\n",
    "\n",
    "# Log output to notebook/stdout\n",
    "stream_handler = logging.StreamHandler(sys.stdout)\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s %(name)s - %(message)s')\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "logger.info(\"Logger initialized and working in Databricks notebook.\")\n",
    "\n",
    "\n",
    "# 2) Configurable Parameters\n",
    "\n",
    "MAILCHIMP_REGION = \"us4\"\n",
    "MAILCHIMP_API_KEY = dbutils.secrets.get(scope=\"MailchimpSpnetwork\", key=\"MailChimp-API-key\")\n",
    "\n",
    "RETENTION_DAYS = 30\n",
    "MAX_LISTS_PAGE_SIZE = 1000\n",
    "PAGE_SIZE = 1000\n",
    "MAX_WORKERS = 5\n",
    "\n",
    "ACCOUNT_NAME = \"mailchimpspnetwork\"\n",
    "BRONZE_CONTAINER = \"bronze\"\n",
    "\n",
    "BRONZE_MAILCHIMP_DIR_BASE = \"mailchimp_members\"\n",
    "\n",
    "\n",
    "# 3) Initialize Azure Data Lake & Mailchimp Clients\n",
    "\n",
    "logger.info(\"Initializing ADLS credentials and client.\")\n",
    "credential = DefaultAzureCredential()\n",
    "service_client = DataLakeServiceClient(\n",
    "    account_url=f\"https://{ACCOUNT_NAME}.dfs.core.windows.net\",\n",
    "    credential=credential\n",
    ")\n",
    "container_client = service_client.get_file_system_client(BRONZE_CONTAINER)\n",
    "logger.info(\"ADLS connection established.\")\n",
    "\n",
    "logger.info(\"Initializing Mailchimp client.\")\n",
    "client = MailchimpMarketing.Client()\n",
    "client.set_config({\n",
    "    \"api_key\": MAILCHIMP_API_KEY,\n",
    "    \"server\": MAILCHIMP_REGION\n",
    "})\n",
    "logger.info(f\"Mailchimp client initialized for region '{MAILCHIMP_REGION}'.\")\n",
    "\n",
    "\n",
    "# 4) Helper Functions\n",
    "\n",
    "def upload_raw_json(folder_path: str, file_name: str, raw_json: str):\n",
    "    file_path = f\"{folder_path}/{file_name}\"\n",
    "    file_client = container_client.get_file_client(file_path)\n",
    "    file_client.upload_data(raw_json.encode(\"utf-8\"), overwrite=True)\n",
    "    logger.debug(f\"Uploaded {file_name} to ADLS at {folder_path}\")\n",
    "\n",
    "\n",
    "def cleanup_old_files():\n",
    "    logger.info(f\"Cleaning up files older than {RETENTION_DAYS} days.\")\n",
    "    cutoff_time = datetime.datetime.utcnow() - datetime.timedelta(days=RETENTION_DAYS)\n",
    "    \n",
    "    paths = container_client.get_paths(path=BRONZE_MAILCHIMP_DIR_BASE, recursive=True)\n",
    "    delete_count = 0\n",
    "    \n",
    "    for path in paths:\n",
    "        if not path.is_directory and path.last_modified < cutoff_time:\n",
    "            logger.debug(f\"Deleting {path.name} (Last modified: {path.last_modified})\")\n",
    "            container_client.delete_file(path.name)\n",
    "            delete_count += 1\n",
    "    \n",
    "    logger.info(f\"Cleanup completed. {delete_count} files deleted.\")\n",
    "\n",
    "\n",
    "def fetch_all_lists() -> List[Dict]:\n",
    "    offset = 0\n",
    "    all_lists = []\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            response = client.lists.get_all_lists(count=MAX_LISTS_PAGE_SIZE, offset=offset)\n",
    "        except ApiClientError as error:\n",
    "            logger.error(f\"Failed to fetch lists at offset {offset}: {error.text}\")\n",
    "            break\n",
    "        \n",
    "        lists_page = response.get(\"lists\", [])\n",
    "        total_items = response.get(\"total_items\", len(lists_page))\n",
    "        \n",
    "        all_lists.extend(lists_page)\n",
    "        offset += MAX_LISTS_PAGE_SIZE\n",
    "        \n",
    "        if offset >= total_items:\n",
    "            break\n",
    "\n",
    "    logger.info(f\"Retrieved {len(all_lists)} Mailchimp lists.\")\n",
    "    return all_lists\n",
    "\n",
    "\n",
    "def safe_sanitize_list_name(raw_name: str) -> str:\n",
    "    return re.sub(r\"[^a-zA-Z0-9_-]\", \"_\", raw_name)\n",
    "\n",
    "def delete_existing_files(folder_path: str):\n",
    "    try:\n",
    "        paths = container_client.get_paths(path=folder_path, recursive=False)\n",
    "        for path in paths:\n",
    "            if not path.is_directory:\n",
    "                container_client.delete_file(path.name)\n",
    "                logger.info(f\"Deleted existing file: {path.name}\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not clean folder {folder_path}: {str(e)}\")\n",
    "\n",
    "def fetch_list_members(list_info: Dict, last_run: str = None):\n",
    "    list_name_clean = safe_sanitize_list_name(list_info[\"name\"])\n",
    "    list_id = list_info[\"id\"]\n",
    "\n",
    "    ingestion_date_str = datetime.datetime.utcnow().strftime(\"%Y-%m-%d\")\n",
    "    folder_path = f\"{BRONZE_MAILCHIMP_DIR_BASE}/listName={list_name_clean}/ingestion_date={ingestion_date_str}\"\n",
    "\n",
    "    logger.info(f\"Cleaning up existing files for list '{list_name_clean}' in {folder_path}\")\n",
    "    delete_existing_files(folder_path)\n",
    "\n",
    "    offset = 0\n",
    "    page_num = 1\n",
    "\n",
    "    logger.info(f\"Fetching ALL members for list '{list_info['name']}' (ID: {list_id})\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            response = client.lists.get_list_members_info(\n",
    "                list_id=list_id,\n",
    "                count=PAGE_SIZE,\n",
    "                offset=offset\n",
    "            )\n",
    "        except ApiClientError as error:\n",
    "            logger.error(f\"Failed to fetch members for {list_name_clean} at offset {offset}: {error.text}\")\n",
    "            break\n",
    "\n",
    "        members = response.get(\"members\", [])\n",
    "        if not members:\n",
    "            logger.info(f\"No members returned for {list_name_clean} at offset {offset}. Ending pagination.\")\n",
    "            break\n",
    "\n",
    "        timestamp_str = datetime.datetime.utcnow().strftime('%Y%m%d%H%M%S')\n",
    "        file_name = f\"{list_name_clean}_page{page_num}_{timestamp_str}.json\"\n",
    "        upload_raw_json(folder_path, file_name, json.dumps(members))\n",
    "\n",
    "        logger.info(f\"Uploaded page {page_num} with {len(members)} members for list '{list_name_clean}'.\")\n",
    "\n",
    "        if len(members) < PAGE_SIZE:\n",
    "            logger.info(f\"Reached last page for list '{list_name_clean}'.\")\n",
    "            break\n",
    "\n",
    "        offset += PAGE_SIZE\n",
    "        page_num += 1\n",
    "\n",
    "\n",
    "\n",
    "def fetch_and_store_members(all_lists: List[Dict]):\n",
    "    logger.info(f\"Fetching members for {len(all_lists)} lists in parallel (max_workers={MAX_WORKERS}).\")\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        futures = [executor.submit(fetch_list_members, lst) for lst in all_lists]\n",
    "        for f in futures:\n",
    "            try:\n",
    "                f.result()\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in parallel fetch: {e}\\n{traceback.format_exc()}\")\n",
    "    logger.info(\"All lists processed in parallel.\")\n",
    "\n",
    "\n",
    "# 5) Main Orchestrator\n",
    "\n",
    "def run_mailchimp_bronze_etl():\n",
    "    logger.info(\"Mailchimp ETL started (FULL extraction, no incrementals).\")\n",
    "\n",
    "    # Fetch all Mailchimp lists\n",
    "    all_lists = fetch_all_lists()\n",
    "    if not all_lists:\n",
    "        logger.warning(\"No lists found in Mailchimp. Exiting early.\")\n",
    "        return\n",
    "\n",
    "    # Fetch and store members (no incremental filter)\n",
    "    fetch_and_store_members(all_lists)\n",
    "\n",
    "    # Cleanup old files\n",
    "    cleanup_old_files()\n",
    "\n",
    "    logger.info(\"Mailchimp ETL Process Completed Successfully.\")\n",
    "\n",
    "\n",
    "# 6) Final Cell: Execute the ETL (Databricks doesn't use __main__)\n",
    "run_mailchimp_bronze_etl()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Bronze_prod",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
