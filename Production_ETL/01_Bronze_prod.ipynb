{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8a000cc-78ea-4289-8e66-fbeea46d2a52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Mailchimp Bronze ETL Notebook\n",
    "\n",
    "This notebook orchestrates a full extraction of Mailchimp lists and their members.\n",
    "It connects to Mailchimp's API, fetches all lists and members, uploads the data to Azure Data Lake Storage,\n",
    "and finally performs cleanup of old files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49a6469c-0987-454f-be68-07cf336666d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# mailchimp_bronze_etl.ipynb\n",
    "\n",
    "# 1) Notebook Configuration & Setup\n",
    "\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import re\n",
    "import logging\n",
    "import sys\n",
    "import traceback\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List, Dict\n",
    "\n",
    "import mailchimp_marketing as MailchimpMarketing\n",
    "from mailchimp_marketing.api_client import ApiClientError\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "\n",
    "# Set up the logger\n",
    "logger = logging.getLogger(\"MailchimpBronzeETL\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Prevent duplicate handlers if re-run\n",
    "if logger.hasHandlers():\n",
    "    logger.handlers.clear()\n",
    "\n",
    "# Log output to notebook/stdout with a specific format\n",
    "stream_handler = logging.StreamHandler(sys.stdout)\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s %(name)s - %(message)s')\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "logger.info(\"Logger initialized and working in Databricks notebook.\")\n",
    "\n",
    "\n",
    "# 2) Configurable Parameters\n",
    "\n",
    "MAILCHIMP_REGION = \"us4\"  # Mailchimp server region\n",
    "# Get the API key from the secret store (Databricks dbutils)\n",
    "MAILCHIMP_API_KEY = dbutils.secrets.get(scope=\"MailchimpSpnetwork\", key=\"MailChimp-API-key\")\n",
    "\n",
    "RETENTION_DAYS = 30  # Days to keep files before cleanup\n",
    "MAX_LISTS_PAGE_SIZE = 1000  # Maximum number of lists to fetch per page\n",
    "PAGE_SIZE = 1000  # Number of list members to fetch per page\n",
    "MAX_WORKERS = 5  # Maximum number of parallel threads for fetching data\n",
    "\n",
    "ACCOUNT_NAME = \"mailchimpspnetwork\"  # Azure Data Lake account name\n",
    "BRONZE_CONTAINER = \"bronze\"  # Azure container name for bronze-level data\n",
    "\n",
    "BRONZE_MAILCHIMP_DIR_BASE = \"mailchimp_members\"  # Base directory for storing Mailchimp data in ADLS\n",
    "\n",
    "\n",
    "\n",
    "# ## 3) Initialize Azure Data Lake & Mailchimp Clients\n",
    "#\n",
    "# We create authenticated clients for Azure Data Lake Storage and Mailchimp API.\n",
    "\n",
    "logger.info(\"Initializing ADLS credentials and client.\")\n",
    "# Use DefaultAzureCredential for Azure authentication\n",
    "credential = DefaultAzureCredential()\n",
    "# Create a DataLakeServiceClient using the ADLS account URL and credential\n",
    "service_client = DataLakeServiceClient(\n",
    "    account_url=f\"https://{ACCOUNT_NAME}.dfs.core.windows.net\",\n",
    "    credential=credential\n",
    ")\n",
    "# Get a client for the bronze container\n",
    "container_client = service_client.get_file_system_client(BRONZE_CONTAINER)\n",
    "logger.info(\"ADLS connection established.\")\n",
    "\n",
    "logger.info(\"Initializing Mailchimp client.\")\n",
    "# Initialize the Mailchimp client with the provided API key and region\n",
    "client = MailchimpMarketing.Client()\n",
    "client.set_config({\n",
    "    \"api_key\": MAILCHIMP_API_KEY,\n",
    "    \"server\": MAILCHIMP_REGION\n",
    "})\n",
    "logger.info(f\"Mailchimp client initialized for region '{MAILCHIMP_REGION}'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "526e44af-6a24-4828-b213-144aa1d939f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##  Helper Functions\n",
    "\n",
    "The following functions support various tasks in the ETL:\n",
    "- **upload_raw_json:** Uploads JSON data to a specified folder in ADLS.\n",
    "- **cleanup_old_files:** Deletes files older than a retention threshold.\n",
    "- **fetch_all_lists:** Retrieves all Mailchimp lists.\n",
    "- **safe_sanitize_list_name:** Cleans list names to be file system friendly.\n",
    "- **delete_existing_files:** Removes files from a folder before new data is written.\n",
    "- **fetch_list_members:** Fetches members for a given list, handling pagination and storing results.\n",
    "- **fetch_and_store_members:** Runs the member extraction in parallel across lists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb6d7c1d-cc19-4f56-921d-6bfbf27a320d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def cleanup_old_folders():\n",
    "    logger.info(f\"Cleaning up ingestion folders older than {RETENTION_DAYS} days.\")\n",
    "    cutoff_time = datetime.datetime.utcnow() - datetime.timedelta(days=RETENTION_DAYS)\n",
    "\n",
    "    # Track each ingestion_date folder's latest modified file\n",
    "    folder_last_modified_map = {}\n",
    "    paths = container_client.get_paths(path=BRONZE_MAILCHIMP_DIR_BASE, recursive=True)\n",
    "\n",
    "    for path in paths:\n",
    "        if path.is_directory:\n",
    "            continue\n",
    "        # Example path: mailchimp_members/listName=XYZ/ingestion_date=2025-03-27/file.json\n",
    "        parts = path.name.split('/')\n",
    "        if len(parts) < 3:\n",
    "            continue\n",
    "\n",
    "        # Identify the folder path: listName=X/ingestion_date=Y\n",
    "        folder_path = '/'.join(parts[:3])\n",
    "        last_modified = path.last_modified\n",
    "\n",
    "        if folder_path not in folder_last_modified_map:\n",
    "            folder_last_modified_map[folder_path] = last_modified\n",
    "        else:\n",
    "            folder_last_modified_map[folder_path] = max(folder_last_modified_map[folder_path], last_modified)\n",
    "\n",
    "    delete_count = 0\n",
    "\n",
    "    for folder_path, last_modified in folder_last_modified_map.items():\n",
    "        if last_modified < cutoff_time:\n",
    "            logger.info(f\"Deleting folder '{folder_path}' (Last modified: {last_modified})\")\n",
    "\n",
    "            # Delete all files in this folder\n",
    "            folder_files = [p for p in paths if p.name.startswith(folder_path + '/') and not p.is_directory]\n",
    "            for file in folder_files:\n",
    "                try:\n",
    "                    container_client.delete_file(file.name)\n",
    "                    logger.debug(f\"Deleted file: {file.name}\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to delete file: {file.name} â€” {e}\")\n",
    "\n",
    "            # Try deleting the directory itself (optional, since ADLS Gen2 may manage \"folders\" virtually)\n",
    "            try:\n",
    "                container_client.delete_directory(folder_path)\n",
    "                logger.debug(f\"Deleted directory: {folder_path}\")\n",
    "            except Exception as e:\n",
    "                logger.debug(f\"Skipped directory deletion (virtual folders): {e}\")\n",
    "\n",
    "            delete_count += 1\n",
    "\n",
    "    logger.info(f\"Cleanup completed. {delete_count} folders deleted.\")\n",
    "\n",
    "\n",
    "    # Delete folders where the most recent file is older than cutoff\n",
    "    for folder, last_modified in folder_last_modified_map.items():\n",
    "        if last_modified < cutoff_time:\n",
    "            logger.debug(f\"Deleting folder: {folder} (Last modified: {last_modified})\")\n",
    "            # Delete all files in the folder\n",
    "            folder_files = [p for p in paths if p.name.startswith(folder + '/') and not p.is_directory]\n",
    "            for file in folder_files:\n",
    "                container_client.delete_file(file.name)\n",
    "            delete_count += 1\n",
    "\n",
    "    logger.info(f\"Folder cleanup completed. {delete_count} folders deleted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2f657e5-d844-4ebe-872f-8f760f6d8ef7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def upload_raw_json(folder_path: str, file_name: str, raw_json: str):\n",
    "    # Construct the file path in ADLS\n",
    "    file_path = f\"{folder_path}/{file_name}\"\n",
    "    # Get a client for the file and upload the JSON data\n",
    "    file_client = container_client.get_file_client(file_path)\n",
    "    file_client.upload_data(raw_json.encode(\"utf-8\"), overwrite=True)\n",
    "    logger.debug(f\"Uploaded {file_name} to ADLS at {folder_path}\")\n",
    "\n",
    "def fetch_all_lists() -> List[Dict]:\n",
    "    offset = 0\n",
    "    all_lists = []\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # Fetch a page of Mailchimp lists starting at the current offset\n",
    "            response = client.lists.get_all_lists(count=MAX_LISTS_PAGE_SIZE, offset=offset)\n",
    "        except ApiClientError as error:\n",
    "            logger.error(f\"Failed to fetch lists at offset {offset}: {error.text}\")\n",
    "            break\n",
    "        \n",
    "        # Extract lists from the response\n",
    "        lists_page = response.get(\"lists\", [])\n",
    "        total_items = response.get(\"total_items\", len(lists_page))\n",
    "        \n",
    "        all_lists.extend(lists_page)\n",
    "        offset += MAX_LISTS_PAGE_SIZE\n",
    "        \n",
    "        # Stop if all lists have been retrieved\n",
    "        if offset >= total_items:\n",
    "            break\n",
    "\n",
    "    logger.info(f\"Retrieved {len(all_lists)} Mailchimp lists.\")\n",
    "    return all_lists\n",
    "\n",
    "def safe_sanitize_list_name(raw_name: str) -> str:\n",
    "    # Replace any characters that are not alphanumeric, underscore, or hyphen with an underscore\n",
    "    return re.sub(r\"[^a-zA-Z0-9_-]\", \"_\", raw_name)\n",
    "\n",
    "def delete_existing_files(folder_path: str):\n",
    "    try:\n",
    "        # Get paths in the specified folder (non-recursive)\n",
    "        paths = container_client.get_paths(path=folder_path, recursive=False)\n",
    "        for path in paths:\n",
    "            if not path.is_directory:\n",
    "                container_client.delete_file(path.name)\n",
    "                logger.info(f\"Deleted existing file: {path.name}\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not clean folder {folder_path}: {str(e)}\")\n",
    "\n",
    "def fetch_list_members(list_info: Dict, last_run: str = None):\n",
    "    # Clean the list name to be used in file paths\n",
    "    list_name_clean = safe_sanitize_list_name(list_info[\"name\"])\n",
    "    list_id = list_info[\"id\"]\n",
    "\n",
    "    # Set the ingestion date to the current UTC date\n",
    "    ingestion_date_str = datetime.datetime.utcnow().strftime(\"%Y-%m-%d\")\n",
    "    folder_path = f\"{BRONZE_MAILCHIMP_DIR_BASE}/listName={list_name_clean}/ingestion_date={ingestion_date_str}\"\n",
    "\n",
    "    logger.info(f\"Cleaning up existing files for list '{list_name_clean}' in {folder_path}\")\n",
    "    delete_existing_files(folder_path)\n",
    "\n",
    "    offset = 0\n",
    "    page_num = 1\n",
    "\n",
    "    logger.info(f\"Fetching ALL members for list '{list_info['name']}' (ID: {list_id})\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Fetch a page of list members using pagination\n",
    "            response = client.lists.get_list_members_info(\n",
    "                list_id=list_id,\n",
    "                count=PAGE_SIZE,\n",
    "                offset=offset\n",
    "            )\n",
    "        except ApiClientError as error:\n",
    "            logger.error(f\"Failed to fetch members for {list_name_clean} at offset {offset}: {error.text}\")\n",
    "            break\n",
    "\n",
    "        members = response.get(\"members\", [])\n",
    "        if not members:\n",
    "            logger.info(f\"No members returned for {list_name_clean} at offset {offset}. Ending pagination.\")\n",
    "            break\n",
    "\n",
    "        # Generate a timestamp for the file name to avoid collisions\n",
    "        timestamp_str = datetime.datetime.utcnow().strftime('%Y%m%d%H%M%S')\n",
    "        file_name = f\"{list_name_clean}_page{page_num}_{timestamp_str}.json\"\n",
    "        # Upload the JSON data for the current page\n",
    "        upload_raw_json(folder_path, file_name, json.dumps(members))\n",
    "\n",
    "        logger.info(f\"Uploaded page {page_num} with {len(members)} members for list '{list_name_clean}'.\")\n",
    "\n",
    "        # If the page returned fewer members than PAGE_SIZE, it's the last page\n",
    "        if len(members) < PAGE_SIZE:\n",
    "            logger.info(f\"Reached last page for list '{list_name_clean}'.\")\n",
    "            break\n",
    "\n",
    "        offset += PAGE_SIZE  # Move to the next page offset\n",
    "        page_num += 1\n",
    "\n",
    "def fetch_and_store_members(all_lists: List[Dict]):\n",
    "    logger.info(f\"Fetching members for {len(all_lists)} lists in parallel (max_workers={MAX_WORKERS}).\")\n",
    "    # Use ThreadPoolExecutor for parallel processing of multiple lists\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        futures = [executor.submit(fetch_list_members, lst) for lst in all_lists]\n",
    "        # Wait for all parallel tasks to complete, and log any errors\n",
    "        for f in futures:\n",
    "            try:\n",
    "                f.result()\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in parallel fetch: {e}\\n{traceback.format_exc()}\")\n",
    "    logger.info(\"All lists processed in parallel.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a359ff1-cb6a-4ffe-a30b-6c041bae9d95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Main Orchestrator\n",
    "\n",
    "The function `run_mailchimp_bronze_etl()` ties all the steps together:\n",
    "1. Fetch all Mailchimp lists.\n",
    "2. For each list, fetch and store all member data in parallel.\n",
    "3. Clean up old files from Azure Data Lake.\n",
    "\n",
    "It logs the start and end of the process along with any issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "536314e1-3d10-4853-b38f-866292c3c633",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_mailchimp_bronze_etl():\n",
    "    logger.info(\"Mailchimp ETL started (FULL extraction, no incrementals).\")\n",
    "\n",
    "    # Fetch all Mailchimp lists\n",
    "    all_lists = fetch_all_lists()\n",
    "    if not all_lists:\n",
    "        logger.warning(\"No lists found in Mailchimp. Exiting early.\")\n",
    "        return\n",
    "    cleanup_old_folders()\n",
    "\n",
    "    # Fetch and store members (no incremental filter)\n",
    "    fetch_and_store_members(all_lists)\n",
    "\n",
    "\n",
    "    logger.info(\"Mailchimp ETL Process Completed Successfully.\")\n",
    "\n",
    "# Execute the ETL process\n",
    "run_mailchimp_bronze_etl()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Bronze_prod",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
