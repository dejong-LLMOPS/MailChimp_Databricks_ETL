{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4aec35c7-669f-4194-ba86-8eb144d0524d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# mailchimp_silver_etl.ipynb\n",
    "# Bronze â†’ Silver ETL pipeline with partition-aware reading and improved logging\n",
    "\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import logging\n",
    "import datetime\n",
    "import re\n",
    "import traceback\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Azure\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.storage.filedatalake import DataLakeServiceClient, FileSystemClient\n",
    "from azure.core.exceptions import ResourceNotFoundError\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1) Spark & Logging Setup (Databricks-Friendly)\n",
    "# ------------------------------------------------------------------------------\n",
    "spark = SparkSession.builder.appName(\"MailchimpSilverETL\").getOrCreate()\n",
    "\n",
    "logger = logging.getLogger(\"MailchimpSilverETL\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "if logger.hasHandlers():\n",
    "    logger.handlers.clear()\n",
    "\n",
    "import sys\n",
    "stream_handler = logging.StreamHandler(sys.stdout)\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s - %(message)s')\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "logger.info(\"Logger initialized successfully.\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2) Configuration\n",
    "# ------------------------------------------------------------------------------\n",
    "STORAGE_ACCOUNT_NAME = \"mailchimpspnetwork\"\n",
    "\n",
    "BRONZE_CONTAINER = \"bronze\"\n",
    "SILVER_CONTAINER = \"silver\"\n",
    "\n",
    "BRONZE_PREFIX = \"mailchimp_members\"\n",
    "SILVER_PREFIX = \"mailchimp_clean\"\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 3) Initialize ADLS\n",
    "# ------------------------------------------------------------------------------\n",
    "logger.info(\"Authenticating with Azure and initializing ADLS...\")\n",
    "credential = DefaultAzureCredential()\n",
    "service_client = DataLakeServiceClient(\n",
    "    account_url=f\"https://{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net\",\n",
    "    credential=credential\n",
    ")\n",
    "bronze_fs = service_client.get_file_system_client(BRONZE_CONTAINER)\n",
    "silver_fs = service_client.get_file_system_client(SILVER_CONTAINER)\n",
    "logger.info(\"Connected to ADLS. Containers mounted successfully.\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 4) Ensure Silver Directory Exists\n",
    "# ------------------------------------------------------------------------------\n",
    "def ensure_directory_exists(fs_client: FileSystemClient, directory_name: str):\n",
    "    dir_client = fs_client.get_directory_client(directory_name)\n",
    "    try:\n",
    "        dir_client.get_directory_properties()\n",
    "        logger.info(f\"Directory exists: {directory_name}\")\n",
    "    except ResourceNotFoundError:\n",
    "        logger.info(f\"Creating directory: {directory_name}\")\n",
    "        fs_client.create_directory(directory_name)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 5) List Mailchimp Lists (from Bronze layout)\n",
    "# ------------------------------------------------------------------------------\n",
    "def list_mailchimp_lists() -> List[str]:\n",
    "    list_names = set()\n",
    "    logger.info(f\"Scanning for Mailchimp lists under '{BRONZE_PREFIX}'...\")\n",
    "\n",
    "    try:\n",
    "        paths = bronze_fs.get_paths(path=BRONZE_PREFIX, recursive=True)\n",
    "    except ResourceNotFoundError:\n",
    "        logger.warning(f\"No path found at '{BRONZE_PREFIX}'\")\n",
    "        return []\n",
    "\n",
    "    for path in paths:\n",
    "        if path.is_directory and \"listName=\" in path.name:\n",
    "            parts = path.name.split('/')\n",
    "            for part in parts:\n",
    "                if part.startswith(\"listName=\"):\n",
    "                    list_name = part.replace(\"listName=\", \"\")\n",
    "                    list_names.add(list_name)\n",
    "\n",
    "    return sorted(list_names)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 6) Read and Merge All JSON Pages for a List\n",
    "# ------------------------------------------------------------------------------\n",
    "def read_bronze_json(list_name: str) -> pd.DataFrame:\n",
    "    logger.info(f\"Reading all JSONs for list '{list_name}'...\")\n",
    "\n",
    "    base_path = f\"{BRONZE_PREFIX}/listName={list_name}\"\n",
    "    all_records = []\n",
    "\n",
    "    try:\n",
    "        paths = bronze_fs.get_paths(path=base_path, recursive=True)\n",
    "    except ResourceNotFoundError:\n",
    "        logger.warning(f\"No data found under '{base_path}'\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    for p in paths:\n",
    "        if p.is_directory or not p.name.lower().endswith(\".json\"):\n",
    "            continue\n",
    "        try:\n",
    "            file_client = bronze_fs.get_file_client(p.name)\n",
    "            content = file_client.download_file().readall()\n",
    "            records = json.loads(content)\n",
    "            if isinstance(records, dict):\n",
    "                records = [records]\n",
    "            all_records.extend(records)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to parse JSON in {p.name}: {e}\")\n",
    "\n",
    "    logger.info(f\"Loaded {len(all_records)} raw records for '{list_name}'\")\n",
    "    df = pd.DataFrame(all_records)\n",
    "\n",
    "    # # Optional: extract unique list_id if present in records\n",
    "    # if \"list_id\" in df.columns:\n",
    "    #     list_id = df[\"list_id\"].dropna().unique()\n",
    "    #     logger.info(f\"Detected list_id(s): {list_id}\")\n",
    "    #     if len(list_id) == 1:\n",
    "    #         df[\"list_id\"] = list_id[0]\n",
    "    #     # If multiple IDs, keep as-is\n",
    "\n",
    "    # Inject list_name for lineage\n",
    "    df[\"list_name\"] = list_name\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 7) Flatten and Clean Member Records\n",
    "# ------------------------------------------------------------------------------\n",
    "def flatten_and_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    logger.info(\"Flattening and cleaning member data...\")\n",
    "\n",
    "    standard_cols = [\n",
    "        \"id\", \"email_address\", \"unique_email_id\", \"contact_id\", \"full_name\",\n",
    "        \"web_id\", \"email_type\", \"status\", \"consents_to_one_to_one_messaging\",\n",
    "        \"ip_signup\", \"timestamp_signup\", \"ip_opt\", \"timestamp_opt\",\n",
    "        \"member_rating\", \"last_changed\", \"language\", \"vip\", \"email_client\",\n",
    "        \"source\"\n",
    "    ]\n",
    "\n",
    "    def flatten_row(row: dict) -> dict:\n",
    "        if \"merge_fields\" in row and isinstance(row[\"merge_fields\"], dict):\n",
    "            for k, v in row[\"merge_fields\"].items():\n",
    "                row[f\"merge_{k}\"] = v\n",
    "            row.pop(\"merge_fields\", None)\n",
    "\n",
    "        if \"location\" in row and isinstance(row[\"location\"], dict):\n",
    "            for k, v in row[\"location\"].items():\n",
    "                row[f\"location_{k}\"] = v\n",
    "            row.pop(\"location\", None)\n",
    "\n",
    "        if \"stats\" in row and isinstance(row[\"stats\"], dict):\n",
    "            for k, v in row[\"stats\"].items():\n",
    "                row[f\"stats_{k}\"] = v\n",
    "            row.pop(\"stats\", None)\n",
    "\n",
    "        return row\n",
    "\n",
    "    flattened = [flatten_row(r) for r in df.to_dict(orient=\"records\")]\n",
    "    df_flat = pd.DataFrame(flattened)\n",
    "\n",
    "    keep_cols = set(standard_cols)\n",
    "    keep_cols.update([col for col in df_flat.columns if col.startswith((\"merge_\", \"location_\", \"stats_\"))])\n",
    "    ordered = [\"list_name\"] + sorted(c for c in df_flat.columns if c not in [\"list_name\"])\n",
    "    df_flat = df_flat[[c for c in ordered if c in df_flat.columns]]\n",
    "\n",
    "    if \"email_address\" in df_flat.columns and \"last_changed\" in df_flat.columns:\n",
    "        df_flat[\"last_changed\"] = pd.to_datetime(df_flat[\"last_changed\"], errors=\"coerce\")\n",
    "        df_flat.sort_values(by=\"last_changed\", ascending=False, inplace=True)\n",
    "        df_flat.drop_duplicates(subset=[\"email_address\"], keep=\"first\", inplace=True)\n",
    "\n",
    "    df_flat.reset_index(drop=True, inplace=True)\n",
    "    logger.info(f\"Flattened DataFrame shape: {df_flat.shape}\")\n",
    "    return df_flat\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 8) Write to Silver (overwrite)\n",
    "# ------------------------------------------------------------------------------\n",
    "def write_to_silver(df: pd.DataFrame, list_name: str):\n",
    "    output_path = f\"{SILVER_PREFIX}/{list_name}.csv\"\n",
    "    buffer = io.StringIO()\n",
    "    df.to_csv(buffer, index=False)\n",
    "\n",
    "    file_client = silver_fs.get_file_client(output_path)\n",
    "    file_client.upload_data(buffer.getvalue(), overwrite=True)\n",
    "    logger.info(f\"Wrote {len(df)} cleaned records to Silver: {output_path}\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 9) Orchestrator: Process All Lists\n",
    "# ------------------------------------------------------------------------------\n",
    "def process_all_lists():\n",
    "    ensure_directory_exists(silver_fs, SILVER_PREFIX)\n",
    "\n",
    "    list_names = list_mailchimp_lists()\n",
    "    logger.info(f\"Found {len(list_names)} list(s): {list_names}\")\n",
    "\n",
    "    for list_name in list_names:\n",
    "        logger.info(f\"Processing list: {list_name}\")\n",
    "        df_bronze = read_bronze_json(list_name)\n",
    "\n",
    "        if df_bronze.empty:\n",
    "            logger.warning(f\"No data found for '{list_name}', skipping.\")\n",
    "            continue\n",
    "\n",
    "        df_clean = flatten_and_clean(df_bronze)\n",
    "        write_to_silver(df_clean, list_name)\n",
    "\n",
    "    logger.info(\"âœ… Bronze-to-Silver ETL complete.\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 10) Execute\n",
    "# ------------------------------------------------------------------------------\n",
    "process_all_lists()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Silver_prod",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
