{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a2896f0-92ee-4c94-aad1-daad27020e69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Mailchimp Silver ETL Notebook\n",
    "\n",
    "This notebook implements the Bronze â†’ Silver ETL pipeline. It reads raw JSON data files from the Bronze layer in ADLS,\n",
    "flattens and cleans the Mailchimp member records, and then writes the processed data as CSV files into the Silver layer.\n",
    "The pipeline includes partition-aware reading, improved logging, and directory management in Azure Data Lake Storage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4aec35c7-669f-4194-ba86-8eb144d0524d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "import logging\n",
    "import datetime\n",
    "import re\n",
    "import traceback\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Azure\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.storage.filedatalake import DataLakeServiceClient, FileSystemClient\n",
    "from azure.core.exceptions import ResourceNotFoundError\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1) Spark & Logging Setup (Databricks-Friendly)\n",
    "# ------------------------------------------------------------------------------\n",
    "spark = SparkSession.builder.appName(\"MailchimpSilverETL\").getOrCreate()\n",
    "\n",
    "logger = logging.getLogger(\"MailchimpSilverETL\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "if logger.hasHandlers():\n",
    "    logger.handlers.clear()\n",
    "\n",
    "import sys\n",
    "stream_handler = logging.StreamHandler(sys.stdout)\n",
    "formatter = logging.Formatter('%(asctime)s %(levelname)s - %(message)s')\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "logger.info(\"Logger initialized successfully.\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2) Configuration\n",
    "# ------------------------------------------------------------------------------\n",
    "STORAGE_ACCOUNT_NAME = \"mailchimpspnetwork\"\n",
    "\n",
    "BRONZE_CONTAINER = \"bronze\"\n",
    "SILVER_CONTAINER = \"silver\"\n",
    "\n",
    "BRONZE_PREFIX = \"mailchimp_members\"\n",
    "SILVER_PREFIX = \"mailchimp_clean\"\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 3) Initialize ADLS\n",
    "# ------------------------------------------------------------------------------\n",
    "logger.info(\"Authenticating with Azure and initializing ADLS...\")\n",
    "credential = DefaultAzureCredential()\n",
    "service_client = DataLakeServiceClient(\n",
    "    account_url=f\"https://{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net\",\n",
    "    credential=credential\n",
    ")\n",
    "bronze_fs = service_client.get_file_system_client(BRONZE_CONTAINER)\n",
    "silver_fs = service_client.get_file_system_client(SILVER_CONTAINER)\n",
    "logger.info(\"Connected to ADLS. Containers mounted successfully.\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 4) Ensure Silver Directory Exists\n",
    "# ------------------------------------------------------------------------------\n",
    "def ensure_directory_exists(fs_client: FileSystemClient, directory_name: str):\n",
    "    dir_client = fs_client.get_directory_client(directory_name)\n",
    "    try:\n",
    "        dir_client.get_directory_properties()\n",
    "        logger.info(f\"Directory exists: {directory_name}\")\n",
    "    except ResourceNotFoundError:\n",
    "        logger.info(f\"Creating directory: {directory_name}\")\n",
    "        fs_client.create_directory(directory_name)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 5) List Mailchimp Lists (from Bronze layout)\n",
    "# ------------------------------------------------------------------------------\n",
    "def list_mailchimp_lists() -> List[str]:\n",
    "    list_names = set()\n",
    "    logger.info(f\"Scanning for Mailchimp lists under '{BRONZE_PREFIX}'...\")\n",
    "\n",
    "    try:\n",
    "        paths = bronze_fs.get_paths(path=BRONZE_PREFIX, recursive=True)\n",
    "    except ResourceNotFoundError:\n",
    "        logger.warning(f\"No path found at '{BRONZE_PREFIX}'\")\n",
    "        return []\n",
    "\n",
    "    for path in paths:\n",
    "        if path.is_directory and \"listName=\" in path.name:\n",
    "            parts = path.name.split('/')\n",
    "            for part in parts:\n",
    "                if part.startswith(\"listName=\"):\n",
    "                    list_name = part.replace(\"listName=\", \"\")\n",
    "                    list_names.add(list_name)\n",
    "\n",
    "    return sorted(list_names)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 6) Read and Merge All JSON Pages for a List\n",
    "# ------------------------------------------------------------------------------\n",
    "def read_bronze_json(list_name: str) -> pd.DataFrame:\n",
    "    logger.info(f\"Reading latest ingestion JSONs for list '{list_name}'...\")\n",
    "\n",
    "    base_path = f\"{BRONZE_PREFIX}/listName={list_name}\"\n",
    "    try:\n",
    "        paths = list(bronze_fs.get_paths(path=base_path, recursive=False))\n",
    "    except ResourceNotFoundError:\n",
    "        logger.warning(f\"No path found at '{base_path}'\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Filter ingestion_date folders and get the most recent one by modified time\n",
    "    ingestion_dirs = [\n",
    "        p for p in paths if p.is_directory and \"ingestion_date=\" in p.name\n",
    "    ]\n",
    "    if not ingestion_dirs:\n",
    "        logger.warning(f\"No ingestion folders found for '{list_name}'\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    latest_dir = max(ingestion_dirs, key=lambda p: p.last_modified)\n",
    "    logger.info(f\"Using latest ingestion folder: {latest_dir.name}\")\n",
    "\n",
    "    # Read JSONs within that latest folder\n",
    "    all_records = []\n",
    "    try:\n",
    "        files = bronze_fs.get_paths(path=latest_dir.name, recursive=True)\n",
    "        for f in files:\n",
    "            if f.is_directory or not f.name.lower().endswith(\".json\"):\n",
    "                continue\n",
    "            try:\n",
    "                file_client = bronze_fs.get_file_client(f.name)\n",
    "                content = file_client.download_file().readall()\n",
    "                records = json.loads(content)\n",
    "                if isinstance(records, dict):\n",
    "                    records = [records]\n",
    "                all_records.extend(records)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to parse JSON in {f.name}: {e}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error accessing files in {latest_dir.name}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    logger.info(f\"Loaded {len(all_records)} records from {latest_dir.name}\")\n",
    "    df = pd.DataFrame(all_records)\n",
    "    df[\"list_name\"] = list_name\n",
    "    return df\n",
    "\n",
    "\n",
    "    # # Optional: extract unique list_id if present in records\n",
    "    # if \"list_id\" in df.columns:\n",
    "    #     list_id = df[\"list_id\"].dropna().unique()\n",
    "    #     logger.info(f\"Detected list_id(s): {list_id}\")\n",
    "    #     if len(list_id) == 1:\n",
    "    #         df[\"list_id\"] = list_id[0]\n",
    "    #     # If multiple IDs, keep as-is\n",
    "\n",
    "    # Inject list_name for lineage\n",
    "    df[\"list_name\"] = list_name\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 7) Flatten and Clean Member Records\n",
    "# ------------------------------------------------------------------------------\n",
    "def flatten_and_clean(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    logger.info(\"Flattening and cleaning member data...\")\n",
    "\n",
    "    standard_cols = [\n",
    "        \"id\", \"email_address\", \"unique_email_id\", \"contact_id\", \"full_name\",\n",
    "        \"web_id\", \"email_type\", \"status\", \"consents_to_one_to_one_messaging\",\n",
    "        \"ip_signup\", \"timestamp_signup\", \"ip_opt\", \"timestamp_opt\",\n",
    "        \"member_rating\", \"last_changed\", \"language\", \"vip\", \"email_client\",\n",
    "        \"source\"\n",
    "    ]\n",
    "\n",
    "    def flatten_row(row: dict) -> dict:\n",
    "        if \"merge_fields\" in row and isinstance(row[\"merge_fields\"], dict):\n",
    "            for k, v in row[\"merge_fields\"].items():\n",
    "                row[f\"merge_{k}\"] = v\n",
    "            row.pop(\"merge_fields\", None)\n",
    "\n",
    "        if \"location\" in row and isinstance(row[\"location\"], dict):\n",
    "            for k, v in row[\"location\"].items():\n",
    "                row[f\"location_{k}\"] = v\n",
    "            row.pop(\"location\", None)\n",
    "\n",
    "        if \"stats\" in row and isinstance(row[\"stats\"], dict):\n",
    "            for k, v in row[\"stats\"].items():\n",
    "                row[f\"stats_{k}\"] = v\n",
    "            row.pop(\"stats\", None)\n",
    "\n",
    "        return row\n",
    "\n",
    "    flattened = [flatten_row(r) for r in df.to_dict(orient=\"records\")]\n",
    "    df_flat = pd.DataFrame(flattened)\n",
    "\n",
    "    keep_cols = set(standard_cols)\n",
    "    keep_cols.update([col for col in df_flat.columns if col.startswith((\"merge_\", \"location_\", \"stats_\"))])\n",
    "    ordered = [\"list_name\"] + sorted(c for c in df_flat.columns if c not in [\"list_name\"])\n",
    "    df_flat = df_flat[[c for c in ordered if c in df_flat.columns]]\n",
    "\n",
    "    if \"email_address\" in df_flat.columns and \"last_changed\" in df_flat.columns:\n",
    "        df_flat[\"last_changed\"] = pd.to_datetime(df_flat[\"last_changed\"], errors=\"coerce\")\n",
    "        df_flat.sort_values(by=\"last_changed\", ascending=False, inplace=True)\n",
    "        df_flat.drop_duplicates(subset=[\"email_address\"], keep=\"first\", inplace=True)\n",
    "\n",
    "    df_flat.reset_index(drop=True, inplace=True)\n",
    "    logger.info(f\"Flattened DataFrame shape: {df_flat.shape}\")\n",
    "    return df_flat\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 8) Write to Silver (overwrite)\n",
    "# ------------------------------------------------------------------------------\n",
    "def write_to_silver(df: pd.DataFrame, list_name: str):\n",
    "    output_path = f\"{SILVER_PREFIX}/{list_name}.csv\"\n",
    "    buffer = io.StringIO()\n",
    "    df.to_csv(buffer, index=False)\n",
    "\n",
    "    file_client = silver_fs.get_file_client(output_path)\n",
    "    file_client.upload_data(buffer.getvalue(), overwrite=True)\n",
    "    logger.info(f\"Wrote {len(df)} cleaned records to Silver: {output_path}\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 9) Orchestrator: Process All Lists\n",
    "# ------------------------------------------------------------------------------\n",
    "def process_all_lists():\n",
    "    ensure_directory_exists(silver_fs, SILVER_PREFIX)\n",
    "\n",
    "    list_names = list_mailchimp_lists()\n",
    "    logger.info(f\"Found {len(list_names)} list(s): {list_names}\")\n",
    "\n",
    "    for list_name in list_names:\n",
    "        logger.info(f\"Processing list: {list_name}\")\n",
    "        df_bronze = read_bronze_json(list_name)\n",
    "\n",
    "        if df_bronze.empty:\n",
    "            logger.warning(f\"No data found for '{list_name}', skipping.\")\n",
    "            continue\n",
    "\n",
    "        df_clean = flatten_and_clean(df_bronze)\n",
    "        write_to_silver(df_clean, list_name)\n",
    "\n",
    "    logger.info(\"Bronze-to-Silver ETL complete.\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 10) Execute\n",
    "# ------------------------------------------------------------------------------\n",
    "process_all_lists()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Silver_prod",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
