{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06ffcdfc-3cd7-4529-a2d8-1bbc0c1c967e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Mailchimp Transformed Data ETL\n",
    "\n",
    "This notebook cell is designed to process CSV files stored in the Silver container of Azure Data Lake Storage (ADLS). It reads raw Mailchimp cleaned data (from the Silver layer), applies standard cleaning and schema enforcement, and writes the transformed data to Delta tables.\n",
    "\n",
    "Below is a detailed breakdown of each section in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe353a6c-636b-44cc-8b19-c8cc8abf5e42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "from azure.core.exceptions import ResourceExistsError\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, concat_ws, udf, expr, lit\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "\n",
    "STORAGE_ACCOUNT_NAME = \"mailchimpspnetwork\"  # Make sure this is all lowercase\n",
    "CONTAINER_NAME = \"silver\"\n",
    "INPUT_PREFIX = \"mailchimp_clean\"\n",
    "OUTPUT_PREFIX = \"mailchimp_transformed\"\n",
    "\n",
    "# ADLS base path for Spark\n",
    "base_path = f\"abfss://{CONTAINER_NAME}@{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net\"\n",
    "\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    service_client = DataLakeServiceClient(\n",
    "        account_url=f\"https://{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net\",\n",
    "        credential=credential\n",
    "    )\n",
    "    fs_client = service_client.get_file_system_client(file_system=CONTAINER_NAME)\n",
    "    print(f\"Connected to ADLS Gen2 container: '{CONTAINER_NAME}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to ADLS: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8225dba-be87-4bc4-9634-e529b3eed78d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Schema Definition\n",
    "\n",
    "- **Standard Schema:**  \n",
    "  Defines the expected schema (`standard_schema`) as a `StructType` for the cleaned/transformed data.  \n",
    "  This schema includes fields such as contact details, email attributes, and flattened address fields (extracted from nested fields in the raw data).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b41377c-2076-4507-9e6b-d9f8bf7cf65f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Schema Definition (including flattened address fields --happens during cleaning since they are nested)\n",
    "\n",
    "standard_schema = StructType([\n",
    "    StructField(\"list_name\", StringType(), True),\n",
    "    StructField(\"consents_to_one_to_one_messaging\", BooleanType(), True),\n",
    "    StructField(\"contact_id\", StringType(), True),\n",
    "    StructField(\"email_address\", StringType(), True),\n",
    "    StructField(\"email_client\", StringType(), True),\n",
    "    StructField(\"email_type\", StringType(), True),\n",
    "    StructField(\"full_name\", StringType(), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"ip_opt\", StringType(), True),\n",
    "    StructField(\"ip_signup\", StringType(), True),\n",
    "    StructField(\"language\", StringType(), True),\n",
    "    StructField(\"last_changed\", StringType(), True),\n",
    "    StructField(\"list_id\", StringType(), True),\n",
    "    StructField(\"location_country_code\", StringType(), True),\n",
    "    StructField(\"location_dstoff\", IntegerType(), True),\n",
    "    StructField(\"location_gmtoff\", IntegerType(), True),\n",
    "    StructField(\"location_latitude\", DoubleType(), True),\n",
    "    StructField(\"location_longitude\", DoubleType(), True),\n",
    "    StructField(\"location_region\", StringType(), True),\n",
    "    StructField(\"location_timezone\", StringType(), True),\n",
    "    StructField(\"member_rating\", IntegerType(), True),\n",
    "    StructField(\"merge_FNAME\", StringType(), True),\n",
    "    StructField(\"merge_LNAME\", StringType(), True),\n",
    "    StructField(\"merge_MMERGE5\", StringType(), True),\n",
    "    StructField(\"merge_PHONE\", StringType(), True),\n",
    "    StructField(\"address_addr1\", StringType(), True),\n",
    "    StructField(\"address_addr2\", StringType(), True),\n",
    "    StructField(\"address_city\", StringType(), True),\n",
    "    StructField(\"address_state\", StringType(), True),\n",
    "    StructField(\"address_zip\", StringType(), True),\n",
    "    StructField(\"address_country\", StringType(), True),\n",
    "    StructField(\"sms_phone_number\", StringType(), True),\n",
    "    StructField(\"sms_subscription_last_updated\", StringType(), True),\n",
    "    StructField(\"sms_subscription_status\", StringType(), True),\n",
    "    StructField(\"source\", StringType(), True),\n",
    "    StructField(\"stats_avg_click_rate\", DoubleType(), True),\n",
    "    StructField(\"stats_avg_open_rate\", DoubleType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"tags_count\", IntegerType(), True),\n",
    "    StructField(\"timestamp_opt\", StringType(), True),\n",
    "    StructField(\"timestamp_signup\", StringType(), True),\n",
    "    StructField(\"unique_email_id\", StringType(), True),\n",
    "    StructField(\"vip\", BooleanType(), True),\n",
    "    StructField(\"web_id\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "630100a0-7011-4c95-b30a-ff70f5248a79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "### Standard Cleaning Function\n",
    "\n",
    "- **Function `standard_cleaning(df)`:**  \n",
    "  - Checks for a nested field (`merge_ADDRESS`) and, if present as a struct, extracts subfields (address components) into individual columns.  \n",
    "  - Drops the original nested field.\n",
    "  - Drops additional nested fields (_links, interests, tags, marketing_permissions, last_note) that are not needed in the final output.\n",
    "  - Ensures the DataFrame includes all fields from the defined standard schema by adding any missing fields (casting them to the correct data type).\n",
    "  - Finally, returns a DataFrame with columns in the order specified by the schema.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92cb5e9b-8365-4940-a904-9fd4b39e8edb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def standard_cleaning(df):\n",
    "    if \"merge_ADDRESS\" in df.columns:\n",
    "        field_type = [f.dataType for f in df.schema.fields if f.name == \"merge_ADDRESS\"][0]\n",
    "        if isinstance(field_type, StructType):\n",
    "            df = df \\\n",
    "                .withColumn(\"address_addr1\", col(\"merge_ADDRESS\").getItem(\"addr1\")) \\\n",
    "                .withColumn(\"address_addr2\", col(\"merge_ADDRESS\").getItem(\"addr2\")) \\\n",
    "                .withColumn(\"address_city\", col(\"merge_ADDRESS\").getItem(\"city\")) \\\n",
    "                .withColumn(\"address_state\", col(\"merge_ADDRESS\").getItem(\"state\")) \\\n",
    "                .withColumn(\"address_zip\", col(\"merge_ADDRESS\").getItem(\"zip\")) \\\n",
    "                .withColumn(\"address_country\", col(\"merge_ADDRESS\").getItem(\"country\")) \\\n",
    "                .drop(\"merge_ADDRESS\")\n",
    "        else:\n",
    "            # If it's not a struct, just drop it\n",
    "            df = df.drop(\"merge_ADDRESS\")\n",
    "\n",
    "    # Drop nested fields not in schema\n",
    "    to_drop = [\"_links\", \"interests\", \"tags\", \"marketing_permissions\", \"last_note\"]\n",
    "    df = df.drop(*[col_name for col_name in to_drop if col_name in df.columns])\n",
    "\n",
    "    # Enforce schema by adding missing fields\n",
    "    for field in standard_schema.fields:\n",
    "        if field.name not in df.columns:\n",
    "            df = df.withColumn(field.name, lit(None).cast(field.dataType))\n",
    "        else:\n",
    "            df = df.withColumn(field.name, col(field.name).cast(field.dataType))\n",
    "\n",
    "    return df.select([f.name for f in standard_schema.fields])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4055d0a9-6b54-402d-b91a-e94066970912",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def list_csvs(path_prefix):\n",
    "    try:\n",
    "        paths = fs_client.get_paths(path_prefix)\n",
    "        return [p.name for p in paths if not p.is_directory and p.name.endswith(\".csv\")]\n",
    "    except ResourceExistsError as e:\n",
    "        print(f\"Error: Path not found {e}\")\n",
    "        return []\n",
    "csv_file_paths = list_csvs(INPUT_PREFIX)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fde77927-2705-40c8-ae1b-08118031cbe9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Processing Each CSV File\n",
    "\n",
    "For each CSV file found:\n",
    "- **Path Setup:**  \n",
    "  - Extracts the file name and constructs full input and output paths using the ADLS base path.\n",
    "  \n",
    "- **Data Reading:**  \n",
    "  - Reads the CSV file into a Spark DataFrame (`df_raw`), and logs its schema and row count.\n",
    "  \n",
    "- **Data Cleaning:**  \n",
    "  - Applies the `standard_cleaning` function to transform and enforce the schema on the raw data.\n",
    "  - Logs the number of rows after cleaning and identifies any missing columns compared to the expected schema.\n",
    "  \n",
    "- **Data Writing:**  \n",
    "  - Writes the cleaned DataFrame to a Delta table at the specified output path (`OUTPUT_PREFIX`) with `overwrite` mode.\n",
    "  - Prints success messages upon completion or error messages if any exceptions occur during processing.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0004814-bb6b-4d5b-9d04-9cd7d0d51610",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Assuming spark is already instantiated\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "for path in csv_file_paths:\n",
    "    file_name = path.split(\"/\")[-1]\n",
    "    input_path = f\"{base_path}/{path}\"\n",
    "    output_path = f\"{base_path}/{OUTPUT_PREFIX}/{file_name.replace('.csv', '')}\"\n",
    "\n",
    "    print(f\"\\nProcessing: **{file_name}**\")\n",
    "    \n",
    "    try:\n",
    "        # Read CSV with the predefined schema and cache the DataFrame to avoid repeated scans.\n",
    "        df_raw = spark.read \\\n",
    "            .option(\"header\", True) \\\n",
    "            .option(\"multiLine\", True) \\\n",
    "            .schema(standard_schema) \\\n",
    "            .csv(input_path) \\\n",
    "            .cache()\n",
    "        \n",
    "        raw_count = df_raw.count()\n",
    "        print(f\"Raw schema: {[f.name for f in df_raw.schema.fields]}\")\n",
    "        print(f\"Raw row count: {raw_count}\")\n",
    "\n",
    "        # Cleaning process (assumed to be optimized)\n",
    "        df_clean = standard_cleaning(df_raw).cache()\n",
    "        cleaned_count = df_clean.count()\n",
    "\n",
    "        # Validate missing expected columns\n",
    "        raw_cols = set(df_raw.columns)\n",
    "        expected_cols = set([f.name for f in standard_schema.fields])\n",
    "        missing_cols = expected_cols - raw_cols\n",
    "\n",
    "        if missing_cols:\n",
    "            print(f\"Missing expected columns: {sorted(missing_cols)}\")\n",
    "\n",
    "        print(f\"Cleaned row count: {cleaned_count}\")\n",
    "        \n",
    "        # Optionally adjust partitions to control output file size/number.\n",
    "        df_to_write = df_clean.coalesce(1)\n",
    "\n",
    "        print(f\"Saving to Delta: {output_path}\")\n",
    "        df_to_write.write.format(\"delta\").mode(\"overwrite\").save(output_path)\n",
    "        \n",
    "        print(f\"Success: {file_name} → {OUTPUT_PREFIX}\")\n",
    "\n",
    "        # Free up the cached DataFrames\n",
    "        df_raw.unpersist()\n",
    "        df_clean.unpersist()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_name}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_Silver_Transformation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
