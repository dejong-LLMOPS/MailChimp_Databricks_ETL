{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe353a6c-636b-44cc-8b19-c8cc8abf5e42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "from azure.core.exceptions import ResourceExistsError\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, concat_ws, udf, expr, lit\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "\n",
    "STORAGE_ACCOUNT_NAME = \"mailchimpspnetwork\"  # Make sure this is all lowercase\n",
    "CONTAINER_NAME = \"silver\"\n",
    "INPUT_PREFIX = \"mailchimp_clean\"\n",
    "OUTPUT_PREFIX = \"mailchimp_transformed\"\n",
    "\n",
    "# ADLS base path for Spark\n",
    "base_path = f\"abfss://{CONTAINER_NAME}@{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net\"\n",
    "\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    service_client = DataLakeServiceClient(\n",
    "        account_url=f\"https://{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net\",\n",
    "        credential=credential\n",
    "    )\n",
    "    fs_client = service_client.get_file_system_client(file_system=CONTAINER_NAME)\n",
    "    print(f\"Connected to ADLS Gen2 container: '{CONTAINER_NAME}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to ADLS: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b41377c-2076-4507-9e6b-d9f8bf7cf65f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Schema Definition (including flattened address fields --happens during cleaning since they are nested)\n",
    "\n",
    "standard_schema = StructType([\n",
    "    StructField(\"list_name\", StringType(), True),\n",
    "    StructField(\"consents_to_one_to_one_messaging\", BooleanType(), True),\n",
    "    StructField(\"contact_id\", StringType(), True),\n",
    "    StructField(\"email_address\", StringType(), True),\n",
    "    StructField(\"email_client\", StringType(), True),\n",
    "    StructField(\"email_type\", StringType(), True),\n",
    "    StructField(\"full_name\", StringType(), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"ip_opt\", StringType(), True),\n",
    "    StructField(\"ip_signup\", StringType(), True),\n",
    "    StructField(\"language\", StringType(), True),\n",
    "    StructField(\"last_changed\", StringType(), True),\n",
    "    StructField(\"list_id\", StringType(), True),\n",
    "    StructField(\"location_country_code\", StringType(), True),\n",
    "    StructField(\"location_dstoff\", IntegerType(), True),\n",
    "    StructField(\"location_gmtoff\", IntegerType(), True),\n",
    "    StructField(\"location_latitude\", DoubleType(), True),\n",
    "    StructField(\"location_longitude\", DoubleType(), True),\n",
    "    StructField(\"location_region\", StringType(), True),\n",
    "    StructField(\"location_timezone\", StringType(), True),\n",
    "    StructField(\"member_rating\", IntegerType(), True),\n",
    "    StructField(\"merge_FNAME\", StringType(), True),\n",
    "    StructField(\"merge_LNAME\", StringType(), True),\n",
    "    StructField(\"merge_MMERGE5\", StringType(), True),\n",
    "    StructField(\"merge_PHONE\", StringType(), True),\n",
    "    StructField(\"address_addr1\", StringType(), True),\n",
    "    StructField(\"address_addr2\", StringType(), True),\n",
    "    StructField(\"address_city\", StringType(), True),\n",
    "    StructField(\"address_state\", StringType(), True),\n",
    "    StructField(\"address_zip\", StringType(), True),\n",
    "    StructField(\"address_country\", StringType(), True),\n",
    "    StructField(\"sms_phone_number\", StringType(), True),\n",
    "    StructField(\"sms_subscription_last_updated\", StringType(), True),\n",
    "    StructField(\"sms_subscription_status\", StringType(), True),\n",
    "    StructField(\"source\", StringType(), True),\n",
    "    StructField(\"stats_avg_click_rate\", DoubleType(), True),\n",
    "    StructField(\"stats_avg_open_rate\", DoubleType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"tags_count\", IntegerType(), True),\n",
    "    StructField(\"timestamp_opt\", StringType(), True),\n",
    "    StructField(\"timestamp_signup\", StringType(), True),\n",
    "    StructField(\"unique_email_id\", StringType(), True),\n",
    "    StructField(\"vip\", BooleanType(), True),\n",
    "    StructField(\"web_id\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92cb5e9b-8365-4940-a904-9fd4b39e8edb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def standard_cleaning(df):\n",
    "    if \"merge_ADDRESS\" in df.columns:\n",
    "        field_type = [f.dataType for f in df.schema.fields if f.name == \"merge_ADDRESS\"][0]\n",
    "        if isinstance(field_type, StructType):\n",
    "            df = df \\\n",
    "                .withColumn(\"address_addr1\", col(\"merge_ADDRESS\").getItem(\"addr1\")) \\\n",
    "                .withColumn(\"address_addr2\", col(\"merge_ADDRESS\").getItem(\"addr2\")) \\\n",
    "                .withColumn(\"address_city\", col(\"merge_ADDRESS\").getItem(\"city\")) \\\n",
    "                .withColumn(\"address_state\", col(\"merge_ADDRESS\").getItem(\"state\")) \\\n",
    "                .withColumn(\"address_zip\", col(\"merge_ADDRESS\").getItem(\"zip\")) \\\n",
    "                .withColumn(\"address_country\", col(\"merge_ADDRESS\").getItem(\"country\")) \\\n",
    "                .drop(\"merge_ADDRESS\")\n",
    "        else:\n",
    "            # If it's not a struct, just drop it\n",
    "            df = df.drop(\"merge_ADDRESS\")\n",
    "\n",
    "    # Drop nested fields not in schema\n",
    "    to_drop = [\"_links\", \"interests\", \"tags\", \"marketing_permissions\", \"last_note\"]\n",
    "    df = df.drop(*[col_name for col_name in to_drop if col_name in df.columns])\n",
    "\n",
    "    # Enforce schema by adding missing fields\n",
    "    for field in standard_schema.fields:\n",
    "        if field.name not in df.columns:\n",
    "            df = df.withColumn(field.name, lit(None).cast(field.dataType))\n",
    "        else:\n",
    "            df = df.withColumn(field.name, col(field.name).cast(field.dataType))\n",
    "\n",
    "    return df.select([f.name for f in standard_schema.fields])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4055d0a9-6b54-402d-b91a-e94066970912",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def list_csvs(path_prefix):\n",
    "    try:\n",
    "        paths = fs_client.get_paths(path_prefix)\n",
    "        return [p.name for p in paths if not p.is_directory and p.name.endswith(\".csv\")]\n",
    "    except ResourceExistsError as e:\n",
    "        print(f\"Error: Path not found {e}\")\n",
    "        return []\n",
    "csv_file_paths = list_csvs(INPUT_PREFIX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49a2c9d9-5e4c-414a-88d5-ceb6bac5aa1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for path in csv_file_paths:\n",
    "    file_name = path.split(\"/\")[-1]\n",
    "    input_path = f\"{base_path}/{path}\"\n",
    "    output_path = f\"{base_path}/{OUTPUT_PREFIX}/{file_name.replace('.csv', '')}\"\n",
    "\n",
    "    print(f\"\\nProcessing: **{file_name}**\")\n",
    "    \n",
    "    try:\n",
    "        df_raw = spark.read.option(\"header\", True).option(\"multiLine\", True).csv(input_path)\n",
    "        raw_count = df_raw.count()\n",
    "\n",
    "        # Log raw schema and row count\n",
    "        print(f\"Raw schema: {[f.name for f in df_raw.schema.fields]}\")\n",
    "        print(f\"Raw row count: {raw_count}\")\n",
    "\n",
    "        # Cleaning\n",
    "        df_clean = standard_cleaning(df_raw)\n",
    "        cleaned_count = df_clean.count()\n",
    "\n",
    "        # Log validation: missing fields\n",
    "        raw_cols = set(df_raw.columns)\n",
    "        expected_cols = set([f.name for f in standard_schema.fields])\n",
    "        missing_cols = expected_cols - raw_cols\n",
    "\n",
    "        if missing_cols:\n",
    "            print(f\"Missing expected columns: {sorted(missing_cols)}\")\n",
    "\n",
    "        print(f\"Cleaned row count: {cleaned_count}\")\n",
    "        print(f\"Saving to Delta: {output_path}\")\n",
    "\n",
    "        # Write cleaned file\n",
    "        df_clean.write.format(\"delta\").mode(\"overwrite\").save(output_path)\n",
    "\n",
    "        print(f\"Success: {file_name} â†’ {OUTPUT_PREFIX}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_name}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_Silver_Transformation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
